{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD0OILMWWETA"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers gradio torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFRAipqpWExu"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "model_name = \"deepset/xlm-roberta-large-squad2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rS4hbexOWGP4"
      },
      "outputs": [],
      "source": [
        "def answer_question(context, question):\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(start_logits)\n",
        "    answer_end = torch.argmax(end_logits) + 1\n",
        "\n",
        "    answer_ids = inputs[\"input_ids\"][0][answer_start:answer_end]\n",
        "    answer = tokenizer.decode(answer_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    start_prob = torch.softmax(start_logits, dim=1)[0][answer_start]\n",
        "    end_prob = torch.softmax(end_logits, dim=1)[0][answer_end - 1]\n",
        "    confidence = (start_prob * end_prob).item()\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    decoded_context = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
        "\n",
        "    if answer in decoded_context:\n",
        "        highlighted_context = decoded_context.replace(answer, f\"<mark><b>{answer}</b></mark>\")\n",
        "    else:\n",
        "        highlighted_context = decoded_context\n",
        "\n",
        "    return f\" ਉੱਤਰ: {answer}\\nਵਿਸ਼ਵਾਸ-ਪੱਧਰ: {confidence:.2f}\", highlighted_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q8zUXkLPWLRE"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"ਪੰਜਾਬੀ ਪੈਰਾ (Context)\", lines=7, placeholder=\"ਇੱਥੇ ਪੈਰਾ ਦਿਓ...\"),\n",
        "        gr.Textbox(label=\"ਪ੍ਰਸ਼ਨ (Question)\", placeholder=\"ਇੱਥੇ ਪ੍ਰਸ਼ਨ ਲਿਖੋ...\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"ਉੱਤਰ & ਵਿਸ਼ਵਾਸ ਪੱਧਰ\"),\n",
        "        gr.HTML(label=\"ਪੈਰਾ (ਉੱਤਰ ਹਾਈਲਾਈਟ ਹੋਇਆ)\")\n",
        "    ],\n",
        "    title=\"ਪੰਜਾਬੀ ਪ੍ਰਸ਼ਨ ਉੱਤਰਣ ਸਿਸਟਮ\",\n",
        "    description=\"ਪੰਜਾਬੀ ਵਿੱਚ ਸਵਾਲ ਪੁੱਛੋ ਅਤੇ ਉੱਤਰ ਪ੍ਰਾਪਤ ਕਰੋ!\"\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}